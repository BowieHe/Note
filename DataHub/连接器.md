# 概要设计

对于数据的接入新增加一个数据连接器的设计，这个数据连接器在整个CDP设计中会作为一个独立的应用程序，对连接分类，连接器配置信息，数据源账号管理和不同的连接管理。同时支持从外部调用RESTful接口或者接受外部的kafka消息，触发来触发一次数据导入的工作，可能会是一个spark或者flink程序。

当定时器发现有batch任务需要触发，DataConnector服务会发送消息，触发一次spark或者flink任务，执行数据的导入ODS层和Staging层的数据映射，以及后面的Staging层的数据导入DW层。在导入任务结束之后会将batch信息回传给DataConnector服务同步更新本次batch结果。

# 详细设计

## 操作概览

2. 在DataHub中点击数据源菜单下面的连接器菜单
3. 从连接分类中选择一个连接器，如sftp连接器
4. 点击连接器上的“数据导入”按钮，建立数据连接
5. 选择已有数据源账号或重新输入一个数据源账号
6. 选择数据，通过路径的选择或者直接指定特定的路径/API路径
7. 配置字段映射关系
8. 配置连接自动同步信息
9. 给当前设置的数据连接配置一个名字和描述
10. 点击连接查看当前连接下面的batch任务详情，包含每次任务的导入文件数量，成功数量，失败数量，还有本次任务状态



## 连接器，连接器分类，数据源账号介绍和关系

### 连接器分类

由于连接器是一个独立的应用，**而且不同的连接器之间的区别主要是针对的数据源类型的不同进行区分**(数据存储上会存储连接器相关的设置，比如需要的账号信息，触发导入任务需要的数据等等)，因此所有的连接器本身都是对所有的项目空间都是可视的，即在数据存储上不会有项目隔离。

连接器分类有多种，可以按业务类型分，如CRM连接器，电商连接器等，有的按技术类型来份，如云存储连接器，数据库连接器等。一个连接器可以属于多个分类，一个分类中也有多个连接器。

### 连接器

连接器在服务中类似一个独立的存在，其数据源类型可以是API，云储存服务等等，不同的连接器中间主要的区别就是数据源类型。其中一个数据连接器可以归属于多个连接器分类，一个数据连接器分类也可以有多个不同的数据连接器

基于以上设计，为了保证数据维护方便，而且由于两个数据都是属于系统默认级别，不会对不同的租户和用户进行区分，全部都是可见的。因此将连接器设置和连接器分类放在一个JSON格式的meta文件中进行统一管理

比如说，同样的Amazon S3连接器，可以一个Amazon S3连接器可以出现在多个分类中，比如云存储，EC分类中；一个EC分类中也可能出现WeChat连接器和Amazon S3连接器。

### 数据源账号

在一个连接器中，基于连接器不同的数据源类型(OSS，SFTP...)，可以绑定多个不同的数据源账号。一个连接器可以绑定多个数据源类型相同的数据源账号，一个数据源账号只能绑定在数据源类型相同的连接器上，但是不受到数据源分类的限制。

`比如有一个OSS的数据源账号，那这个账号只能绑定在OSS类型的连接器上，但是可以在EC分类中，或者CRM分类中`

此外，在每一次对这个数据源账号创建的时候会进行一次测试连接，检测这个账号是不是可用。同时在每一次数据同步的时候，如果收到当前账号过期的错误信息，也会停止同步并且将当前账号状态修改为*INVALID*

### 关系概括

基于以上设计，一个连接器分类中可以有多个数据连接器，一个数据连接器也可以属于多个连接器分类，这两者属于多对多的关系；

一个数据连接器下可以绑定多个数据源类型相同的数据源账号，但是一个数据源账号只能绑定一个连接器；且数据源账号和连接器分类之间没有强关联关系

## 数据连接相关

在设置好数据源账号，数据连接器和连接器分类之后，可有在这个数据源账号下创建多个数据连接(data connection)，每一个数据连接都会有自己的一套配置信息，包括以下数据
- categoryId(连接器分类类型)
- accountId(数据源账号ID)
- 数据源的文件路径
- **数据源导入的文件类型-比如csv，parquet**
- 映射到的staging层的表ID(对应GDM中 data table 的表ID，而不是表名)
- 字段映射(field mapping)，(如果是API或者kafka导入的流数据通过schema mapping 映射)
- 同步相关设置(cron表达式)
- 数据连接名称和描述
- 上一次同步时间和下一次同步时间

在数据连接创建完成之后，会根据传递的syncMode的值和syncPeriod里面的参数来更新下一次同步时间和同步配置
- syncMode为手动：只更新上一次同步时间，下一次同步时间为空
- syncMode为自动:
	1. 自定义(从某一天起，隔多少天同步一次)
		- 查看syncPeriod中的fromDate字段，如果有则说明是自定义时间。每一次同步任务执行之后对当前时间增加相隔的日期，并更新下一次同步时间
	2. 定期同步
		- 查看根据syncPriod中的week和month计算出cron表达式，并且在每一次同步任务执行之后，根据cron表达是计算出下一次同步时间

## 数据同步任务相关

### 同步任务的触发和记录

在DataConnector服务中会有一个定时器每一分钟触发，去遍历表DataConnection的nextSyncTime，检查是否有连接需要同步更新数据。

如果有则在表ConnectionBatch中新建一条记录，其batchId是当前在connectionBatch表中当前connectionId下最大的batchId+1.

同时通过sparkJobStarter提交sparkJob，通过spark-bootstrap服务触发data-spark-job中的spark-data-inject任务来同步数据。

在完成相关的数据导入之后会通过kakfa消息返回任务状态到ConnectionBatch更新任务状态和记录。

ConnectionBatch的任务同步状态通过status来记录，下面的状态依次对应不同的任务阶段
- (status_start): "任务开始",
- (status_job_submitted): "提交Spark任务",
- (status_job_start): "Spark任务开始执行",
- (status_job_progress): "Spark任务执行中",
- (status_data_loaded): "数据文件加载完成",
- (status_success): "任务结束",
- (status_failed): "任务失败",
- (status_partial_failed): "任务部分失败"

其中状态的更新都通过发送kafka消息来更新connectionBatch表的中的processRecord字段(该字段是一个json格式的list，记录每一次状态的更新时间)
```json
[
  {
    "name": "start",
    "label": "任务开始",
    "date": "2021-03-31T03:27:26Z"
  },
  {
    "name": "job_submitted",
    "label": "提交Spark任务",
    "date": "2021-03-31T03:27:27Z"
  },
  {
    "name": "job_start",
    "label": "Spark任务开始执行",
    "date": "2021-03-31T03:28:27Z"
  },
  {
    "name": "data_loaded",
    "label": "数据文件加载完成",
    "date": "2021-03-31T03:29:27Z"
  }
]
```

### 同步任务的处理

#### 处理概览

在提交spark-job任务之后，spark-job会根据connection配置信息的不同，对于导入的数据处理也会不同：
- connection配置信息中有数据源文件路径和文件类型，并且有mapping_meta，则**数据源文件会同步到ODS层，并且同步到staging层**（比如csv，parquet文件）
- ~~connection配置信息中只有数据源文件路径，没有mapping_meta，则只会**存储原数据到ODS层，不会同步到staging层**（比如视频等无法同步到表的数据，）~~
- connection配置信息中有mapping_meta，没有数据源文件路径，**数据直接同步到staging层，不会存储数据到ODS层**（比如从JDBC或者API过来的数据）

#### ODS层处理

处理第一种任务类型的时候（文件类型是parquet或csv），在导入文件的时候，文件的路径是(/dbName/connectionId/batchId)

由于当前任务的batchId已知，
- 如果batchId是1，说明是第一次导入这个文件的数据：
	- 则直接读取csv或者parquet文件内的信息，转换成dataset并按照parquet文件格式保存，其分区是当前的connection 的batchId
- 如果batchId大于1，说明之前已经有导入任务：
	- 则会读取这个表的schema，和当前文件的转换过后的dataset的schema对比，如果有不同的列则通过kafka消息返回失败，并且警告表结构和之前不同；没有不同的列则字段一一匹配存入这个parquet表

处理第二种类型，由于文件类型不是csv或者parquet，直接存储，创建的文件夹名和前面的parquet表名类似，保存文件的时候会在文件名后面加上一个batchId存储。**不会通过spark处理，直接在DataConnector服务中处理并保存到hdfs**

1. **ODS层文件存储，就是hdfs那样存储？**
2. **ODS层的数据受不受namespace的限制?**
3. **文件名和表字段的中文怎么处理，存储没有问题，但不清楚后续会不会有问题**

#### staging层处理

在处理之前，staging层接受的都是一个dataset数据，因此来源可能是：
1. 第一种任务类型：读取对应的parquet表中的特定分区的文件，转换成一个dataSet传递
2. 第二种任务类型：直接从数据源读取数据，转换成dataset传递

根据触发spark-job时传递的fieldMapping，对传递过来的dataset中的数据一行行进行列转换，并且insert到对应的stagingTable中。

如果中间出现了列转换的错误，会将错误记录下来，并且存储在一个csv文件中，保存在HDFS，记录错误原因，文件名是(connectionId + batchId + ’failed‘.csv)，下载的时候直接通过文件名去hdfs中下载对应文件
